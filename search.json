[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mining Acknowledgements Sections",
    "section": "",
    "text": "blah"
  },
  {
    "objectID": "index.html#original-dataset",
    "href": "index.html#original-dataset",
    "title": "Mining Acknowledgements Sections",
    "section": "Original Dataset",
    "text": "Original Dataset\nThe original dataset of 64 papers was provided to us as a large JSON file that had a lot data within it. For our analysis of acknowledgements sections we only needed a few data points to get started. The original dataset is available below for exploration (minor change just to make it render nicely).\n\n\nShow Code for Loading the Original Dataset\nfrom IPython.display import JSON\nimport json\n\nwith open(\"data/599_lit_review.json\", \"r\") as open_f:\n    original_dataset = json.load(open_f)\n    \nJSON({\"data\": original_dataset})\n\n\n<IPython.core.display.JSON object>"
  },
  {
    "objectID": "index.html#compiled-dataset",
    "href": "index.html#compiled-dataset",
    "title": "Mining Acknowledgements Sections",
    "section": "Compiled Dataset",
    "text": "Compiled Dataset\nFor our analysis, we really only needed some metadata and a view or download link for each paper which we could then manually go and copy-paste any acknowledgements section into our dataset (we have some thoughts as to how to automate this in a later section).\nTo extract the data we needed we ran the following code:\n\n\nShow Code for Compile Dataset for Manual Addition\nimport pandas as pd\n\ncompiled_rows = []\nfor index, paper in enumerate(original_dataset):\n    # Some papers have data from CSL and some from S2\n    # Get both so we don't really have to care later on\n    \n    # Check if the paper has CSL data at all\n    if paper.get(\"csl\", None) is not None:\n        # Find or get title and url returned by CSL data\n        csl_title = paper[\"csl\"].get(\"title\", None)\n        csl_url = paper[\"csl\"].get(\"URL\", None)\n    else:\n        csl_title = None\n        csl_url = None\n\n    # Check if the paper has Semantic Scholar data at all\n    if paper.get(\"s2data\", None) is not None:\n        # Find or get title and url returned by S2 data\n        s2_title = paper[\"s2data\"].get(\"title\", None)\n        s2_url = paper[\"s2data\"].get(\"url\", None)\n    else:\n        s2_title = None\n        s2_url = None\n    \n    # Compile all results\n    compiled_rows.append({\n        \"paper_index\": index,\n        \"doi\": paper[\"doi\"],\n        \"s2id\": paper.get(\"s2id\", None),\n        \"s2_url\": s2_url,\n        \"csl_url\": csl_url,\n        \"s2_title\": s2_title,\n        \"csl_title\": csl_title,\n        \"acknowledgements_text\": None,\n    })\n    \ncompiled_dataset = pd.DataFrame(compiled_rows)\ncompiled_dataset.head()\n\n\n\n\n\n\n  \n    \n      \n      paper_index\n      doi\n      s2id\n      s2_url\n      csl_url\n      s2_title\n      csl_title\n      acknowledgements_text\n    \n  \n  \n    \n      0\n      0\n      10.48550/arXiv.2205.02007\n      d1cf6bafac02ac65c7464bc7b168023584a688d7\n      https://www.semanticscholar.org/paper/d1cf6baf...\n      https://arxiv.org/abs/2205.02007\n      A Computational Inflection for Scientific Disc...\n      A Computational Inflection for Scientific Disc...\n      None\n    \n    \n      1\n      1\n      10.23915/distill.00028\n      e291f920147ae876877928e1be94f62ef732e3c6\n      https://www.semanticscholar.org/paper/e291f920...\n      http://dx.doi.org/10.23915/distill.00028\n      Communicating with Interactive Articles\n      Communicating with Interactive Articles\n      None\n    \n    \n      2\n      2\n      10.1111/cgf.13720\n      fe1ea23231e63bdc8738635046e21d7e655e55f2\n      https://www.semanticscholar.org/paper/fe1ea232...\n      http://dx.doi.org/10.1111/cgf.13720\n      Capture & Analysis of Active Reading Behaviors...\n      Capture &amp; Analysis of Active Reading Behav...\n      None\n    \n    \n      3\n      3\n      10.1093/comjnl/27.2.97\n      1ffa1efffa494807ba40ccf6453d156167caa12f\n      https://www.semanticscholar.org/paper/1ffa1eff...\n      http://dx.doi.org/10.1093/comjnl/27.2.97\n      Literate Programming\n      Literate Programming\n      None\n    \n    \n      4\n      4\n      10.1145/3173574.3173606\n      fcc86317d6a0e5f67968d2e84d20dd5ba0524cf2\n      https://www.semanticscholar.org/paper/fcc86317...\n      http://dx.doi.org/10.1145/3173574.3173606\n      Exploration and Explanation in Computational N...\n      Exploration and Explanation in Computational N...\n      None\n    \n  \n\n\n\n\nOur dataset after adding all the acknowledgements sections is available below:\n\n\nRead and Show Data with Acknowledgements Sections Added\nraw_data = pd.read_csv(\"data/raw-ack-sections.csv\")\nraw_data.head()\n\n\n\n\n\n\n  \n    \n      \n      paper_index\n      doi\n      s2id\n      s2_url\n      csl_url\n      s2_title\n      csl_title\n      acknowledgements_text\n    \n  \n  \n    \n      0\n      0\n      10.48550/arXiv.2205.02007\n      d1cf6bafac02ac65c7464bc7b168023584a688d7\n      https://www.semanticscholar.org/paper/d1cf6baf...\n      https://arxiv.org/abs/2205.02007\n      A Computational Inflection for Scientific Disc...\n      A Computational Inflection for Scientific Disc...\n      We thank the members of the Semantic Scholar t...\n    \n    \n      1\n      1\n      10.23915/distill.00028\n      e291f920147ae876877928e1be94f62ef732e3c6\n      https://www.semanticscholar.org/paper/e291f920...\n      http://dx.doi.org/10.23915/distill.00028\n      Communicating with Interactive Articles\n      Communicating with Interactive Articles\n      We are grateful to Arvind Satyanarayan for his...\n    \n    \n      2\n      2\n      10.1111/cgf.13720\n      fe1ea23231e63bdc8738635046e21d7e655e55f2\n      https://www.semanticscholar.org/paper/fe1ea232...\n      http://dx.doi.org/10.1111/cgf.13720\n      Capture & Analysis of Active Reading Behaviors...\n      Capture &amp; Analysis of Active Reading Behav...\n      NaN\n    \n    \n      3\n      3\n      10.1093/comjnl/27.2.97\n      1ffa1efffa494807ba40ccf6453d156167caa12f\n      https://www.semanticscholar.org/paper/1ffa1eff...\n      http://dx.doi.org/10.1093/comjnl/27.2.97\n      Literate Programming\n      Literate Programming\n      The preparation of this paper was supported in...\n    \n    \n      4\n      4\n      10.1145/3173574.3173606\n      fcc86317d6a0e5f67968d2e84d20dd5ba0524cf2\n      https://www.semanticscholar.org/paper/fcc86317...\n      http://dx.doi.org/10.1145/3173574.3173606\n      Exploration and Explanation in Computational N...\n      Exploration and Explanation in Computational N...\n      We thank Regina Cheng and Nathan Hassanzadeh f..."
  },
  {
    "objectID": "index.html#ner",
    "href": "index.html#ner",
    "title": "Mining Acknowledgements Sections",
    "section": "NER",
    "text": "NER\nWe can now take each of these acknowledgements sections and run them through a named entity recognition model.\nLoad the model and show a few examples of what the model produces.\n\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_trf\")\n\n\n# Filter dataset to only include rows with acknowledgements sections\nfiltered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n\n# Get random sample\nrandom_example_subset = filtered_data.sample(3)\n\n# Example processed docs\ndocs = []\nfor text in random_example_subset.acknowledgements_text:\n    docs.append(nlp(text))\n\n\ndisplacy.render(docs[0], style=\"ent\")\n\nThis research was supported by \n\n    Digital Libraries Initiative\n    ORG\n\n, under grant \n\n    NSF\n    ORG\n\n CA98-17353.\n\n\n\ndisplacy.render(docs[1], style=\"ent\")\n\nThis work was supported by \n\n    Toyota Research Institute\n    ORG\n\n through the \n\n    Accelerated Materials Design and Discovery\n    ORG\n\n program. We thank \n\n    T. Botari\n    PERSON\n\n, \n\n    M. Horton\n    PERSON\n\n, \n\n    D. Mrdjenovich\n    PERSON\n\n, \n\n    N. Mingione\n    PERSON\n\n and \n\n    A. Faghaninia\n    PERSON\n\n for discussions.\n\n\n\ndisplacy.render(docs[2], style=\"ent\")\n\nThe authors would like to thank \n\n    Dorothea Salo\n    PERSON\n\n, \n\n    Kristin Antelman\n    PERSON\n\n, and \n\n    John Sack\n    PERSON\n\n for extensive and valuable comments on a draft of this article. The author order of \n\n    JP\n    ORG\n\n and \n\n    HP\n    ORG\n\n was determined by coin flip, as is their custom.\n\n\nWe can loop over all the rows in the filtered dataset and collect each named entity referenced and it’s label.\n\n# Filter dataset to only include rows with acknowledgements sections\nfiltered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n\n# For each acknowledgement, run it through spacy,\n# extract entities and their labels and store to a dataframe\nentities_rows = []\nfor _, paper in filtered_data.iterrows():\n    doc = nlp(paper.acknowledgements_text)\n    for ent in doc.ents:\n        # Store with the DOI so we can join with other data later\n        entities_rows.append({\n            \"doi\": paper.doi,\n            \"entity\": ent.text,\n            \"entity_label\": ent.label_,\n        })\n        \nentities = pd.DataFrame(entities_rows)\nentities\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n\n\n\n\n\n\n  \n    \n      \n      doi\n      entity\n      entity_label\n    \n  \n  \n    \n      0\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      1\n      10.48550/arXiv.2205.02007\n      2132318\n      CARDINAL\n    \n    \n      2\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      3\n      10.48550/arXiv.2205.02007\n      2040196\n      CARDINAL\n    \n    \n      4\n      10.48550/arXiv.2205.02007\n      ONR\n      ORG\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      398\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      399\n      10.23915/distill.00031\n      the last few years\n      DATE\n    \n    \n      400\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      401\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      402\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n  \n\n403 rows × 3 columns\n\n\n\nHere are the most common entity types:\n\nimport altair as alt\n\nalt.Chart(entities).mark_bar().encode(\n    alt.X(\"entity_label\", sort=\"-y\"),\n    y=\"count()\",\n    color=\"entity_label\",\n    tooltip=[\"entity_label\", \"count()\"],\n).properties(\n    width=400,\n    height=300\n).interactive()\n\n\n\n\n\n\n\nA bulk of the named entities are people and organizations (which is what we would expect and what we are looking for), we can filter out the rest.\n\n# Filter all rows that aren't people or orgs\npeople_and_org_refs = entities.loc[entities.entity_label.isin([\"PERSON\", \"ORG\"])]\n\nThis is still too much data to visualize each person or org’s count so let’s just visualize a the top ten referenced people or entities.\n\ntop_ten_entities = people_and_org_refs.value_counts(\n    subset=[\"entity\", \"entity_label\"]\n).to_frame().reset_index().rename(columns={0: \"count\"})[:10]\n\n\nalt.Chart(top_ten_entities).mark_bar().encode(\n    alt.X(\"entity\", sort=\"-y\"),\n    y=\"count\",\n    color=\"entity\",\n    tooltip=[\"entity\", \"entity_label\", \"count\"],\n).properties(\n    width=400,\n    height=300\n).interactive()"
  },
  {
    "objectID": "index.html#classifying-recognition",
    "href": "index.html#classifying-recognition",
    "title": "Mining Acknowledgements Sections",
    "section": "Classifying Recognition",
    "text": "Classifying Recognition\nblah"
  }
]
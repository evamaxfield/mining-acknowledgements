{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9b5ee167-4893-470b-b815-16a1661d559a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Mining Acknowledgements Sections\"\n",
    "author: \"Eva Maxfield Brown and Chris Fu\"\n",
    "date: \"October 28, 2022\"\n",
    "\n",
    "toc: true\n",
    "toc-location: left\n",
    "format:\n",
    "  html:\n",
    "    code-tools: true\n",
    "    standalone: true\n",
    "    embed-resources: true\n",
    "\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "csl: support/nature.csl\n",
    "\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4cadf-1c9b-4181-968f-3e4450e2d54d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c70102-073f-46ff-92e5-7ffc168a2301",
   "metadata": {},
   "source": [
    "## Original Dataset\n",
    "\n",
    "The original dataset of 64 papers was provided to us as a large JSON file that had a lot data within it. For our analysis of acknowledgements sections we only needed a few data points to get started. The original dataset is available below for exploration (minor change just to make it render nicely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720e243-adb9-4612-a349-545c8c128aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show Code for Loading the Original Dataset\"\n",
    "\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "with open(\"data/599_lit_review.json\", \"r\") as open_f:\n",
    "    original_dataset = json.load(open_f)\n",
    "    \n",
    "JSON({\"data\": original_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f42ca-ca2e-4571-8b79-34bfab11c8d7",
   "metadata": {},
   "source": [
    "## Compiled Dataset\n",
    "\n",
    "For our analysis, we really only needed some metadata and a view or download link for each paper which we could then manually go and copy-paste any acknowledgements section into our dataset (we have some thoughts as to how to automate this in a later section).\n",
    "\n",
    "To extract the data we needed we ran the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8c02-c752-4737-84f4-57500f7f9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show Code for Compile Dataset for Manual Addition\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "compiled_rows = []\n",
    "for index, paper in enumerate(original_dataset):\n",
    "    # Some papers have data from CSL and some from S2\n",
    "    # Get both so we don't really have to care later on\n",
    "    \n",
    "    # Check if the paper has CSL data at all\n",
    "    if paper.get(\"csl\", None) is not None:\n",
    "        # Find or get title and url returned by CSL data\n",
    "        csl_title = paper[\"csl\"].get(\"title\", None)\n",
    "        csl_url = paper[\"csl\"].get(\"URL\", None)\n",
    "    else:\n",
    "        csl_title = None\n",
    "        csl_url = None\n",
    "\n",
    "    # Check if the paper has Semantic Scholar data at all\n",
    "    if paper.get(\"s2data\", None) is not None:\n",
    "        # Find or get title and url returned by S2 data\n",
    "        s2_title = paper[\"s2data\"].get(\"title\", None)\n",
    "        s2_url = paper[\"s2data\"].get(\"url\", None)\n",
    "    else:\n",
    "        s2_title = None\n",
    "        s2_url = None\n",
    "    \n",
    "    # Compile all results\n",
    "    compiled_rows.append({\n",
    "        \"paper_index\": index,\n",
    "        \"doi\": paper[\"doi\"],\n",
    "        \"s2id\": paper.get(\"s2id\", None),\n",
    "        \"s2_url\": s2_url,\n",
    "        \"csl_url\": csl_url,\n",
    "        \"s2_title\": s2_title,\n",
    "        \"csl_title\": csl_title,\n",
    "        \"acknowledgements_text\": None,\n",
    "    })\n",
    "    \n",
    "compiled_dataset = pd.DataFrame(compiled_rows)\n",
    "compiled_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063222c4-7fb2-468b-8e3d-6d6a6446457f",
   "metadata": {},
   "source": [
    "Our dataset after adding all the acknowledgements sections is available below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf675bc-d56e-460e-9948-923a5ab26f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Read and Show Data with Acknowledgements Sections Added\"\n",
    "\n",
    "raw_data = pd.read_csv(\"data/raw-ack-sections.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124badc-76dc-42ff-8930-33eb3c2e4148",
   "metadata": {},
   "source": [
    "## NER\n",
    "\n",
    "We can now take each of these acknowledgements sections and run them through a named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de9e92-a3db-4e72-8c14-d29414ed39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Download the Spacy Model\"\n",
    "#| echo: false\n",
    "\n",
    "# Note there is a larger / more accuract model available with: \"en_core_web_trf\"\n",
    "# Run this outside of Jupyter\n",
    "# python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e7627-e6ad-4149-a182-f51d0b60a94e",
   "metadata": {},
   "source": [
    "Load the model and show a few examples of what the model produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396b2c2-1c12-4bb2-ae38-b5f9b81753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Import Spacy and Load Model\"\n",
    "#| warning: false\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899dedc-34ec-4ab4-8f00-077ef3d11a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Filter and Sample the Dataset for Examples\"\n",
    "#| warning: false\n",
    "\n",
    "# Filter dataset to only include rows with acknowledgements sections\n",
    "filtered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n",
    "\n",
    "# Get random sample\n",
    "random_example_subset = filtered_data.sample(3)\n",
    "\n",
    "# Example processed docs\n",
    "docs = []\n",
    "for text in random_example_subset.acknowledgements_text:\n",
    "    docs.append(nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef219d03-8a2e-4cad-9924-066fe8b5607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docs[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d6017-3445-49b5-bb21-802d39aabb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docs[1], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afdf79-d671-44e4-9ecc-579d22ba2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docs[2], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e48156",
   "metadata": {},
   "source": [
    "We can loop over all the rows in the filtered dataset and collect each named entity referenced and it's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f0fd7-3a3c-4846-8620-cd56f5dee095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Run NER Across the Whole Dataset\"\n",
    "\n",
    "# Filter dataset to only include rows with acknowledgements sections\n",
    "filtered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n",
    "\n",
    "# For each acknowledgement, run it through spacy,\n",
    "# extract entities and their labels and store to a dataframe\n",
    "entities_rows = []\n",
    "for _, paper in filtered_data.iterrows():\n",
    "    doc = nlp(paper.acknowledgements_text)\n",
    "    for ent in doc.ents:\n",
    "        # Store with the DOI so we can join with other data later\n",
    "        entities_rows.append({\n",
    "            \"doi\": paper.doi,\n",
    "            \"entity\": ent.text,\n",
    "            \"entity_label\": ent.label_,\n",
    "        })\n",
    "        \n",
    "entities = pd.DataFrame(entities_rows)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce4fb4",
   "metadata": {},
   "source": [
    "Here are the most common entity types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b8932-7dcb-4613-8d3b-c600884ab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "alt.Chart(entities).mark_bar().encode(\n",
    "    alt.X(\"entity_label\", sort=\"-y\"),\n",
    "    y=\"count()\",\n",
    "    color=\"entity_label\",\n",
    "    tooltip=[\"entity_label\", \"count()\"],\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50705fee",
   "metadata": {},
   "source": [
    "A bulk of the named entities are people and organizations (which is what we would expect and what we are looking for), we can filter out the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a626d1-0ebc-4c37-b09b-2e4b316f543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all rows that aren't people or orgs\n",
    "people_and_org_refs = entities.loc[entities.entity_label.isin([\"PERSON\", \"ORG\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d030ed",
   "metadata": {},
   "source": [
    "This is still too much data to visualize each person or org's count so let's just visualize a the top ten referenced people or entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff96b9-9c9c-4601-81de-b607d5ee3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_entities = people_and_org_refs.value_counts(\n",
    "    subset=[\"entity\", \"entity_label\"]\n",
    ").to_frame().reset_index().rename(columns={0: \"count\"})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f293e23-09af-4e5a-9fb3-002194cc0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "alt.Chart(top_ten_entities).mark_bar().encode(\n",
    "    alt.X(\"entity\", sort=\"-y\"),\n",
    "    y=\"count\",\n",
    "    color=\"entity\",\n",
    "    tooltip=[\"entity\", \"entity_label\", \"count\"],\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84876cf5-b06b-46df-b24d-37e75796a8b9",
   "metadata": {},
   "source": [
    "## Classifying Recognition\n",
    "\n",
    "blah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
